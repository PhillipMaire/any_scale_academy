{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Introduction to the Ray AI Libraries: An example of using Ray data, Ray Train, Ray Tune, Ray Serve to implement a XGBoost regression model\n",
    "\n",
    "Â© 2025, Anyscale. All Rights Reserved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ’» **Launch Locally**: You can run this notebook locally, but performance will be reduced.\n",
    "\n",
    "ðŸš€ **Launch on Cloud**: A Ray Cluster with 4 GPUs (Click [here](http://console.anyscale.com/register) to easily start a Ray cluster on Anyscale) is recommended to run this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with a quick end-to-end example to get a sense of what the Ray AI Libraries can do.\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b> Here is the roadmap for this notebook:</b>\n",
    "<ul>\n",
    "    <li>Overview of the Ray AI Libraries</li>\n",
    "    <li>Quick end-to-end example</li>\n",
    "    <ul>\n",
    "      <li>Vanilla XGBoost code</li>\n",
    "      <li>Hyperparameter tuning with Ray Tune</li>\n",
    "      <li>Distributed training with Ray Train</li>\n",
    "      <li>Serving an ensemble model with Ray Serve</li>\n",
    "      <li>Batch inference with Ray Data</li>\n",
    "    </ul>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l\u001b[K\u001b[34mâ ‹\u001b[0m JSON API formula.jws.json                        [Downloading  31.7MB/-------]\n",
      "\u001b[K\u001b[34mâ ‹\u001b[0m JSON API cask.jws.json                           [Downloading  15.1MB/-------]\u001b[1F\u001b[K\u001b[34mâ ‹\u001b[0m JSON API formula.jws.json                        [Downloading  31.7MB/-------]\n",
      "\u001b[K\u001b[34mâ ‹\u001b[0m JSON API cask.jws.json                           [Downloading  15.1MB/-------]\u001b[1F\u001b[K\u001b[34mâ ™\u001b[0m JSON API formula.jws.json                        [Downloading   1.2MB/-------]\n",
      "\u001b[K\u001b[34mâ ™\u001b[0m JSON API cask.jws.json                           [Downloading   1.1MB/-------]\u001b[1F\u001b[K\u001b[34mâ ™\u001b[0m JSON API formula.jws.json                        [Downloading   8.7MB/-------]\n",
      "\u001b[K\u001b[34mâ ™\u001b[0m JSON API cask.jws.json                           [Downloading  11.6MB/-------]\u001b[1F\u001b[K\u001b[34mâ š\u001b[0m JSON API formula.jws.json                        [Downloading  31.7MB/-------]\n",
      "\u001b[K\u001b[34mâ ž\u001b[0m JSON API cask.jws.json                           [Downloaded   15.1MB/-------]\u001b[1F\u001b[K\u001b[32mâœ”ï¸Ž\u001b[0m JSON API cask.jws.json                           [Downloaded   15.1MB/ 15.1MB]\n",
      "\u001b[K\u001b[34mâ –\u001b[0m JSON API formula.jws.json                        [Downloaded   31.7MB/ 31.7MB]\u001b[0G\u001b[K\u001b[32mâœ”ï¸Ž\u001b[0m JSON API formula.jws.json                        [Downloaded   31.7MB/ 31.7MB]\n",
      "\u001b[?25h\u001b[33mWarning:\u001b[0m libomp 21.1.7 is already installed and up-to-date.\n",
      "To reinstall 21.1.7, run:\n",
      "  brew reinstall libomp\n"
     ]
    }
   ],
   "source": [
    "# (Optional): If you get an XGBoostError at import, you might have to `brew install libomp` before importing xgboost again\n",
    "!brew install libomp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import fastapi\n",
    "import pandas as pd\n",
    "import requests\n",
    "# macos: If you get an XGBoostError at import, you might have to `brew install libomp` before importing xgboost again\n",
    "import xgboost\n",
    "from pydantic import BaseModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import ray\n",
    "import ray.tune\n",
    "import ray.train\n",
    "from ray.train.xgboost import XGBoostTrainer as RayTrainXGBoostTrainer\n",
    "from ray.train import RunConfig\n",
    "import ray.data\n",
    "import ray.serve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Overview of the Ray AI Libraries\n",
    "\n",
    "<img src=\"https://technical-training-assets.s3.us-west-2.amazonaws.com/Ray_AI_Libraries/Ray+AI+Libraries.png\" width=\"700px\" loading=\"lazy\">\n",
    "\n",
    "Built on top of Ray Core, the Ray AI Libraries inherit all the performance and scalability benefits offered by Core while providing a convenient abstraction layer for machine learning. These Python-first native libraries allow ML practitioners to distribute individual workloads, end-to-end applications, and build custom use cases in a unified framework.\n",
    "\n",
    "The Ray AI Libraries bring together an ever-growing ecosystem of integrations with popular machine learning frameworks to create a common interface for development.\n",
    "\n",
    "|<img src=\"https://technical-training-assets.s3.us-west-2.amazonaws.com/Introduction_to_Ray_AIR/e2e_air.png\" width=\"100%\" loading=\"lazy\">|\n",
    "|:-:|\n",
    "|Ray AI Libraries enable end-to-end ML development and provides multiple options for integrating with other tools and libraries from the MLOps ecosystem.|\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Quick end-to-end example\n",
    "\n",
    "For this classification task, you will apply a simple [XGBoost](https://xgboost.readthedocs.io/en/stable/) (a gradient boosted trees framework) model to the June 2021 [New York City Taxi & Limousine Commission's Trip Record Data](https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page). \n",
    "\n",
    "The full dataset contains millions of samples of yellow cab rides, and the goal is to predict the tip amount.\n",
    "\n",
    "**Dataset features**\n",
    "* **`passenger_count`**\n",
    "    * Float (whole number) representing number of passengers.\n",
    "* **`trip_distance`** \n",
    "    * Float representing trip distance in miles.\n",
    "* **`fare_amount`**\n",
    "    * Float representing total price including tax, tip, fees, etc.\n",
    "* **`tolls_amount`**\n",
    "    * Float representing the total paid on tolls if any.\n",
    "\n",
    "**Target**\n",
    "* **`trip_amount`**\n",
    "    * Float representing the total paid as tips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Vanilla XGboost code\n",
    "\n",
    "Let's start with the vanilla XGBoost code to predict the tip amount for a NYC taxi cab data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    \"passenger_count\", \n",
    "    \"trip_distance\",\n",
    "    \"fare_amount\",\n",
    "    \"tolls_amount\",\n",
    "]\n",
    "\n",
    "label_column = \"tip_amount\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to load the data and split into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    path = \"s3://anyscale-public-materials/nyc-taxi-cab/yellow_tripdata_2021-03.parquet\"\n",
    "    df = pd.read_parquet(path, columns=features + [label_column])\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        df[features], df[label_column], test_size=0.2, random_state=42\n",
    "    )\n",
    "    dtrain = xgboost.DMatrix(X_train, label=y_train)\n",
    "    dtest = xgboost.DMatrix(X_test, label=y_test)\n",
    "    return dtrain, dtest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to run `xgboost.train` given some hyperparameter dictionary `params`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_folder = \"/mnt/cluster_storage/\" # Modify this path to your local folder if it runs on your local environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "When getting information for key 'nyc-taxi-cab/yellow_tripdata_2021-03.parquet' in bucket 'anyscale-public-materials': AWS Error ACCESS_DENIED during HeadObject operation: No response body.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 26\u001b[39m\n\u001b[32m     17\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[33m\"\u001b[39m\u001b[33meval-rmse\u001b[39m\u001b[33m\"\u001b[39m: evals_result[\u001b[33m\"\u001b[39m\u001b[33meval\u001b[39m\u001b[33m\"\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mrmse\u001b[39m\u001b[33m\"\u001b[39m][-\u001b[32m1\u001b[39m]}\n\u001b[32m     19\u001b[39m params = {\n\u001b[32m     20\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mobjective\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mreg:squarederror\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     21\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33meval_metric\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mrmse\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     24\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33meta\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m0.1\u001b[39m,\n\u001b[32m     25\u001b[39m }\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m \u001b[43mmy_xgboost_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 6\u001b[39m, in \u001b[36mmy_xgboost_func\u001b[39m\u001b[34m(params)\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmy_xgboost_func\u001b[39m(params):    \n\u001b[32m      5\u001b[39m     evals_result = {}\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     dtrain, dtest = \u001b[43mload_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m     bst = xgboost.train(\n\u001b[32m      8\u001b[39m         params, \n\u001b[32m      9\u001b[39m         dtrain, \n\u001b[32m   (...)\u001b[39m\u001b[32m     12\u001b[39m         evals_result=evals_result,\n\u001b[32m     13\u001b[39m     )\n\u001b[32m     14\u001b[39m     \u001b[38;5;66;03m# Use Path\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 3\u001b[39m, in \u001b[36mload_data\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_data\u001b[39m():\n\u001b[32m      2\u001b[39m     path = \u001b[33m\"\u001b[39m\u001b[33ms3://anyscale-public-materials/nyc-taxi-cab/yellow_tripdata_2021-03.parquet\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mlabel_column\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m     X_train, X_test, y_train, y_test = train_test_split(\n\u001b[32m      5\u001b[39m         df[features], df[label_column], test_size=\u001b[32m0.2\u001b[39m, random_state=\u001b[32m42\u001b[39m\n\u001b[32m      6\u001b[39m     )\n\u001b[32m      7\u001b[39m     dtrain = xgboost.DMatrix(X_train, label=y_train)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/ray-jupyter/lib/python3.11/site-packages/pandas/io/parquet.py:669\u001b[39m, in \u001b[36mread_parquet\u001b[39m\u001b[34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, filesystem, filters, **kwargs)\u001b[39m\n\u001b[32m    666\u001b[39m     use_nullable_dtypes = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    667\u001b[39m check_dtype_backend(dtype_backend)\n\u001b[32m--> \u001b[39m\u001b[32m669\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimpl\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    670\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    671\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    672\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    673\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    674\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    675\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    676\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    677\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    678\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/ray-jupyter/lib/python3.11/site-packages/pandas/io/parquet.py:265\u001b[39m, in \u001b[36mPyArrowImpl.read\u001b[39m\u001b[34m(self, path, columns, filters, use_nullable_dtypes, dtype_backend, storage_options, filesystem, **kwargs)\u001b[39m\n\u001b[32m    258\u001b[39m path_or_handle, handles, filesystem = _get_path_or_handle(\n\u001b[32m    259\u001b[39m     path,\n\u001b[32m    260\u001b[39m     filesystem,\n\u001b[32m    261\u001b[39m     storage_options=storage_options,\n\u001b[32m    262\u001b[39m     mode=\u001b[33m\"\u001b[39m\u001b[33mrb\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    263\u001b[39m )\n\u001b[32m    264\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m265\u001b[39m     pa_table = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapi\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparquet\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_table\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    266\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpath_or_handle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    267\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    268\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    269\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    270\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    271\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    273\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m catch_warnings():\n\u001b[32m    274\u001b[39m         filterwarnings(\n\u001b[32m    275\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    276\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mmake_block is deprecated\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    277\u001b[39m             \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m,\n\u001b[32m    278\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/ray-jupyter/lib/python3.11/site-packages/pyarrow/parquet/core.py:1844\u001b[39m, in \u001b[36mread_table\u001b[39m\u001b[34m(source, columns, use_threads, schema, use_pandas_metadata, read_dictionary, binary_type, list_type, memory_map, buffer_size, partitioning, filesystem, filters, ignore_prefixes, pre_buffer, coerce_int96_timestamp_unit, decryption_properties, thrift_string_size_limit, thrift_container_size_limit, page_checksum_verification, arrow_extensions_enabled)\u001b[39m\n\u001b[32m   1832\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mread_table\u001b[39m(source, *, columns=\u001b[38;5;28;01mNone\u001b[39;00m, use_threads=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m   1833\u001b[39m                schema=\u001b[38;5;28;01mNone\u001b[39;00m, use_pandas_metadata=\u001b[38;5;28;01mFalse\u001b[39;00m, read_dictionary=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1834\u001b[39m                binary_type=\u001b[38;5;28;01mNone\u001b[39;00m, list_type=\u001b[38;5;28;01mNone\u001b[39;00m, memory_map=\u001b[38;5;28;01mFalse\u001b[39;00m, buffer_size=\u001b[32m0\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1840\u001b[39m                page_checksum_verification=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   1841\u001b[39m                arrow_extensions_enabled=\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m   1843\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1844\u001b[39m         dataset = \u001b[43mParquetDataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1845\u001b[39m \u001b[43m            \u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1846\u001b[39m \u001b[43m            \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m=\u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1847\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1848\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpartitioning\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpartitioning\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1849\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1850\u001b[39m \u001b[43m            \u001b[49m\u001b[43mread_dictionary\u001b[49m\u001b[43m=\u001b[49m\u001b[43mread_dictionary\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1851\u001b[39m \u001b[43m            \u001b[49m\u001b[43mbinary_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbinary_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1852\u001b[39m \u001b[43m            \u001b[49m\u001b[43mlist_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlist_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1853\u001b[39m \u001b[43m            \u001b[49m\u001b[43mbuffer_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbuffer_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1854\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1855\u001b[39m \u001b[43m            \u001b[49m\u001b[43mignore_prefixes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_prefixes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1856\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpre_buffer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpre_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1857\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcoerce_int96_timestamp_unit\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcoerce_int96_timestamp_unit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1858\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdecryption_properties\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecryption_properties\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1859\u001b[39m \u001b[43m            \u001b[49m\u001b[43mthrift_string_size_limit\u001b[49m\u001b[43m=\u001b[49m\u001b[43mthrift_string_size_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1860\u001b[39m \u001b[43m            \u001b[49m\u001b[43mthrift_container_size_limit\u001b[49m\u001b[43m=\u001b[49m\u001b[43mthrift_container_size_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1861\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpage_checksum_verification\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpage_checksum_verification\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1862\u001b[39m \u001b[43m            \u001b[49m\u001b[43marrow_extensions_enabled\u001b[49m\u001b[43m=\u001b[49m\u001b[43marrow_extensions_enabled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1863\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1864\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[32m   1865\u001b[39m         \u001b[38;5;66;03m# fall back on ParquetFile for simple cases when pyarrow.dataset\u001b[39;00m\n\u001b[32m   1866\u001b[39m         \u001b[38;5;66;03m# module is not available\u001b[39;00m\n\u001b[32m   1867\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m filters \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/ray-jupyter/lib/python3.11/site-packages/pyarrow/parquet/core.py:1401\u001b[39m, in \u001b[36mParquetDataset.__init__\u001b[39m\u001b[34m(self, path_or_paths, filesystem, schema, filters, read_dictionary, binary_type, list_type, memory_map, buffer_size, partitioning, ignore_prefixes, pre_buffer, coerce_int96_timestamp_unit, decryption_properties, thrift_string_size_limit, thrift_container_size_limit, page_checksum_verification, arrow_extensions_enabled)\u001b[39m\n\u001b[32m   1397\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _is_path_like(path_or_paths):\n\u001b[32m   1398\u001b[39m     filesystem, path_or_paths = _resolve_filesystem_and_path(\n\u001b[32m   1399\u001b[39m         path_or_paths, filesystem, memory_map=memory_map\n\u001b[32m   1400\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1401\u001b[39m     finfo = \u001b[43mfilesystem\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_file_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_paths\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1402\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m finfo.type == FileType.Directory:\n\u001b[32m   1403\u001b[39m         \u001b[38;5;28mself\u001b[39m._base_dir = path_or_paths\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/ray-jupyter/lib/python3.11/site-packages/pyarrow/_fs.pyx:616\u001b[39m, in \u001b[36mpyarrow._fs.FileSystem.get_file_info\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/ray-jupyter/lib/python3.11/site-packages/pyarrow/error.pxi:155\u001b[39m, in \u001b[36mpyarrow.lib.pyarrow_internal_check_status\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/ray-jupyter/lib/python3.11/site-packages/pyarrow/error.pxi:92\u001b[39m, in \u001b[36mpyarrow.lib.check_status\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mOSError\u001b[39m: When getting information for key 'nyc-taxi-cab/yellow_tripdata_2021-03.parquet' in bucket 'anyscale-public-materials': AWS Error ACCESS_DENIED during HeadObject operation: No response body."
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "model_path = Path(storage_folder) / \"model.ubj\"\n",
    "\n",
    "def my_xgboost_func(params):    \n",
    "    evals_result = {}\n",
    "    dtrain, dtest = load_data()\n",
    "    bst = xgboost.train(\n",
    "        params, \n",
    "        dtrain, \n",
    "        num_boost_round=10, \n",
    "        evals=[(dtest, \"eval\")], \n",
    "        evals_result=evals_result,\n",
    "    )\n",
    "    # Use Path\n",
    "    bst.save_model(model_path)\n",
    "    print(f\"{evals_result['eval']}\")\n",
    "    return {\"eval-rmse\": evals_result[\"eval\"][\"rmse\"][-1]}\n",
    "\n",
    "params = {\n",
    "    \"objective\": \"reg:squarederror\",\n",
    "    \"eval_metric\": \"rmse\",\n",
    "    \"tree_method\": \"hist\",\n",
    "    \"max_depth\": 6,\n",
    "    \"eta\": 0.1,\n",
    "}\n",
    "my_xgboost_func(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Hyperparameter tuning with Ray Tune\n",
    "\n",
    "Let's use Ray Tune to run distributed hyperparameter tuning for the XGBoost model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = ray.tune.Tuner(  # Create a tuner\n",
    "    my_xgboost_func,  # Pass it the training function which Ray Tune calls Trainable.\n",
    "    param_space={  # Pass it the parameter space to search over\n",
    "        \"objective\": \"reg:squarederror\",\n",
    "        \"eval_metric\": \"rmse\",\n",
    "        \"tree_method\": \"hist\",\n",
    "        \"max_depth\": 6,\n",
    "        \"eta\": ray.tune.uniform(0.01, 0.3),\n",
    "    },\n",
    "    run_config=RunConfig(storage_path=storage_folder),\n",
    "    tune_config=ray.tune.TuneConfig(  # Tell it which metric to tune\n",
    "        metric=\"eval-rmse\",\n",
    "        mode=\"min\",\n",
    "        num_samples=10,\n",
    "    ),\n",
    ")\n",
    "\n",
    "results = tuner.fit()  # Run the tuning job\n",
    "print(results.get_best_result().config)  # Get back the best hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a diagram that shows what Tune does:\n",
    "\n",
    "It is effectively scheduling many trials and returning the best performing one.\n",
    "\n",
    "<img src=\"https://bair.berkeley.edu/static/blog/tune/tune-arch-simple.png\" width=\"700px\" loading=\"lazy\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Distributed training with Ray Train\n",
    "\n",
    "In case your training data is too large, your training might take a long time to complete.\n",
    "\n",
    "To speed it up, shard the dataset across training workers and perform distributed XGBoost training.\n",
    "\n",
    "Let's redefine `load_data` to now load a different slice of the data given the worker index/rank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    # find out which training worker is running this code\n",
    "    train_ctx = ray.train.get_context()\n",
    "    worker_rank = train_ctx.get_world_rank()\n",
    "    print(f\"Loading data for worker {worker_rank}...\")\n",
    "\n",
    "    # build path based on training worker rank\n",
    "    month = (worker_rank + 1) % 12\n",
    "    year = 2021 + (worker_rank + 1) // 12\n",
    "    path = f\"s3://anyscale-public-materials/nyc-taxi-cab/yellow_tripdata_{year}-{month:02}.parquet\"\n",
    "\n",
    "    # same as before\n",
    "    df = pd.read_parquet(path, columns=features + [label_column])\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        df[features], df[label_column], test_size=0.2, random_state=42\n",
    "    )\n",
    "    dtrain = xgboost.DMatrix(X_train, label=y_train)\n",
    "    dtest = xgboost.DMatrix(X_test, label=y_test)\n",
    "    return dtrain, dtest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can run distributed XGBoost training using Ray Train's XGBoostTrainer - similar trainers exist for other popular ML frameworks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = RayTrainXGBoostTrainer(  # Create a trainer\n",
    "    my_xgboost_func,  # Pass it the training function\n",
    "    scaling_config=ray.train.ScalingConfig(\n",
    "        num_workers=2, use_gpu=False\n",
    "    ),  # Define how many training workers\n",
    "    train_loop_config=params,  # Pass it the hyperparameters\n",
    ")\n",
    "\n",
    "trainer.fit()  # Run the training job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a diagram that shows what Train does:\n",
    "\n",
    "A train controller will create training workers and execute the training function on each worker.\n",
    "\n",
    "Ray Train delegates the distributed training to the underlying XGBoost framework.\n",
    "\n",
    "<img src=\"https://docs.ray.io/en/latest/_images/overview.png\" width=\"700px\" loading=\"lazy\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Serving an ensemble model with Ray Serve\n",
    "\n",
    "Ray Serve allows for distributed serving of models and complex inference pipelines.\n",
    "\n",
    "Here is a diagram showing how to deploy an ensemble model with Ray Serve:\n",
    "\n",
    "<img src=\"https://images.ctfassets.net/xjan103pcp94/3DJ7vVRxYIvcFO7JmIUMCx/77a45caa275ffa46f5135f4d6726dd4f/Figure_2_-_Fanout_and_ensemble.png\" width=\"700px\" loading=\"lazy\">\n",
    "\n",
    "Here is how the resulting code looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app = fastapi.FastAPI()\n",
    "\n",
    "class Payload(BaseModel):\n",
    "    passenger_count: int\n",
    "    trip_distance: float\n",
    "    fare_amount: float\n",
    "    tolls_amount: float\n",
    "\n",
    "\n",
    "@ray.serve.deployment\n",
    "@ray.serve.ingress(app)\n",
    "class Ensemble:\n",
    "    def __init__(self, model1, model2):\n",
    "        self.model1 = model1\n",
    "        self.model2 = model2\n",
    "\n",
    "    @app.post(\"/predict\")\n",
    "    async def predict(self, data: Payload) -> dict:\n",
    "        model1_prediction, model2_prediction = await asyncio.gather(\n",
    "            self.model1.predict.remote([data.model_dump()]),\n",
    "            self.model2.predict.remote([data.model_dump()]),\n",
    "        )\n",
    "        out = {\"prediction\": float(model1_prediction + model2_prediction) / 2}\n",
    "        return out\n",
    "\n",
    "\n",
    "@ray.serve.deployment\n",
    "class Model:\n",
    "    def __init__(self, path: str):\n",
    "        self._model = xgboost.Booster()\n",
    "        self._model.load_model(path)\n",
    "\n",
    "    def predict(self, data: list[dict]) -> list[float]:\n",
    "        # Make prediction\n",
    "        dmatrix = xgboost.DMatrix(pd.DataFrame(data))\n",
    "        model_prediction = self._model.predict(dmatrix)\n",
    "        return model_prediction\n",
    "\n",
    "\n",
    "# Run the deployment\n",
    "handle = ray.serve.run(\n",
    "    Ensemble.bind(\n",
    "        model1=Model.bind(model_path),\n",
    "        model2=Model.bind(model_path),\n",
    "    ),\n",
    "    route_prefix=\"/ensemble\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make an HTTP request to the Ray Serve instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "requests.post(\n",
    "    \"http://localhost:8000/ensemble/predict\",\n",
    "    json={  # Use json parameter instead of params\n",
    "        \"passenger_count\": 1,\n",
    "        \"trip_distance\": 2.5,\n",
    "        \"fare_amount\": 10.0,\n",
    "        \"tolls_amount\": 0.5,\n",
    "    },\n",
    ").json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Batch inference with Ray Data\n",
    "\n",
    "Ray Data allows for distributed data processing through streaming execution across a heterogeneous cluster of CPUs and GPUs.\n",
    "\n",
    "This makes Ray Data ideal for workloads like compute-intensive data processing, data ingestion, and batch inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OfflinePredictor:\n",
    "    def __init__(self):\n",
    "        # Load expensive state\n",
    "        self._model = xgboost.Booster()\n",
    "        self._model.load_model(model_path)\n",
    "\n",
    "    def predict(self, data: list[dict]) -> list[float]:\n",
    "        # Make prediction in batch\n",
    "        dmatrix = xgboost.DMatrix(pd.DataFrame(data))\n",
    "        model_prediction = self._model.predict(dmatrix)\n",
    "        return model_prediction\n",
    "\n",
    "    def __call__(self, batch: dict) -> dict:\n",
    "        batch[\"predictions\"] = self.predict(batch)\n",
    "        return batch\n",
    "\n",
    "\n",
    "# Apply the predictor to the validation dataset\n",
    "prediction_pipeline = (\n",
    "    ray.data.read_parquet(\n",
    "        \"s3://anyscale-public-materials/nyc-taxi-cab/yellow_tripdata_2021-03.parquet\"\n",
    "    )\n",
    "    .select_columns(features)\n",
    "    .map_batches(OfflinePredictor, concurrency=(2, 10))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After defining the pipeline, we can execute it in a distributed manner by writing the output to a sink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_pipeline.write_parquet(\"./xgboost_predictions\") #update this to your local path if runs on your local"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect the produced predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls {storage_folder}/xgboost_predictions/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell for file cleanup \n",
    "!rm -rf {storage_folder}/xgboost_predictions/\n",
    "!rm {model_path}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ray-jupyter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
